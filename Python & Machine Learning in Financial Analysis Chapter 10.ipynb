{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uySfR9fUYM6v"
   },
   "source": [
    "# Deep Learning in Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slLHXsZCYQA2"
   },
   "source": [
    "## Deep Learning for Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "snE59TpWX7CL"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, Subset\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "\n",
    "from chapter_10_utils import performance_evaluation_report, custom_set_seed, create_input_data\n",
    "\n",
    "# Set seed for reproducibility\n",
    "custom_set_seed(42)\n",
    "\n",
    "# Print PyTorch version\n",
    "print(torch.__version__)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [6, 2.5]\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# Ignore future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCFgk_gpYkoJ"
   },
   "source": [
    "2. Load the dataset from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "-ha7DQwcYoAX",
    "outputId": "b7373d8d-f0a4-4787-dd71-3cd054cd0037"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('credit_card_default.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMCqKlnBYwrY"
   },
   "source": [
    "3. Identify the dependent variable (target) and numerical/categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4sgSZkBY22B"
   },
   "outputs": [],
   "source": [
    "DEP_VAR = 'default_payment_next_month'\n",
    "\n",
    "num_features = list(df.select_dtypes('number').columns)\n",
    "num_features.remove(DEP_VAR)\n",
    "cat_features = list(df.select_dtypes('object').columns)\n",
    "\n",
    "preprocessing = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pCyL_cEZKoh"
   },
   "source": [
    "4. Create a `TabularDataBunch` from the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kH--0SZ0ZO5j"
   },
   "outputs": [],
   "source": [
    "data = (TabularList.from_df(df, \n",
    "                            cat_names=cat_features,\n",
    "                            cont_names=num_features, \n",
    "                            procs=preprocessing)\n",
    "                   .split_by_rand_pct(valid_pct=0.2, seed=42)\n",
    "                   .label_from_df(cols=DEP_VAR)\n",
    "                   .databunch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nycZ775ZheX"
   },
   "source": [
    "We additionally inspect a few rows from the DataBunch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "tyqLbPc7ZmL4",
    "outputId": "13a6adc5-2dad-43ab-cbfb-68dce294b9e3"
   },
   "outputs": [],
   "source": [
    "data.show_batch(rows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4DUJHbKZqdr"
   },
   "source": [
    "5. Define the `Learner` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjRedzSlZtj9"
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(data, layers=[1000,500],\n",
    "                        ps=[0.001,0.01],\n",
    "                        emb_drop=0.04,\n",
    "                        metrics=[Recall(),\n",
    "                                 FBeta(beta=1),\n",
    "                                 FBeta(beta=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUhXoSZqaBZe"
   },
   "source": [
    "6. Inspect the model's architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQeM2XmEaE2m",
    "outputId": "dfd0a345-30c2-4c2e-80a8-0c6494c48261"
   },
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVQYoYiLaIlo"
   },
   "source": [
    "`Embedding(11, 6)` means that a categorical embedding was created with 11 input values and 6 output latent features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKVyCVUkaRLb"
   },
   "source": [
    "7. Find the suggested learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "id": "P-U9s126aUlj",
    "outputId": "8b3badf0-63c8-4684-ff23-fd2accedb4c8"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wimyB98afwo"
   },
   "source": [
    "8. Train the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "9UZHO3mwaijA",
    "outputId": "8a61257b-8512-4fcb-de36-45cbdce72ff6"
   },
   "outputs": [],
   "source": [
    "learn.fit(epochs=25, lr=1e-6, wd=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp8dhGwCbXP5"
   },
   "source": [
    "9. Plot the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "CXETDLOtbZbk",
    "outputId": "dd5461a1-40c7-4e66-a379-27d20395c847"
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vFNMaIcbi5a"
   },
   "source": [
    "10. Extract the predictions for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Z0qqRnTHbnTo",
    "outputId": "092e3bb9-7dff-419f-9fdb-f4f7e57e8962"
   },
   "outputs": [],
   "source": [
    "preds_valid, _ = learn.get_preds(ds_type=DatasetType.Valid)\n",
    "pred_valid = preds_valid.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPeTqIljbyH1"
   },
   "source": [
    "11. Inspect the performance (confusion matrix) on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "95naCbA0b3Zd",
    "outputId": "5bc6c133-29c4-4b9a-be73-0a8900e870e7"
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "Bwvc8LM9cFBF",
    "outputId": "5621afdc-91b5-4184-ff5f-1d763d5824e1"
   },
   "outputs": [],
   "source": [
    "interp.plot_tab_top_losses(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXnHINzqcLGS"
   },
   "source": [
    "12. Inspect the performance evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "DB_6jcZRcPFq",
    "outputId": "3595a362-2c22-44cf-8285-fddc502dd9de"
   },
   "outputs": [],
   "source": [
    "performance_evaluation_report(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD-X1AeamQ0f"
   },
   "source": [
    "## Multilayer perceptrons for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKcV3K-Jm8n-"
   },
   "source": [
    "2. Define parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpqpSTUfm_QW"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "TICKER = 'ANF'\n",
    "START_DATE = '2010-01-02'\n",
    "END_DATE = '2019-12-31'\n",
    "N_LAGS = 3\n",
    "\n",
    "# neural network \n",
    "VALID_SIZE = 12\n",
    "BATCH_SIZE = 5\n",
    "N_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ4nL7p5nMnw"
   },
   "source": [
    "3. Download and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qc9sVEhjnQDS"
   },
   "outputs": [],
   "source": [
    "df = yf.download(TICKER, \n",
    "                 start=START_DATE, \n",
    "                 end=END_DATE,\n",
    "                 progress=False)\n",
    "\n",
    "df = df.resample('M').last()\n",
    "prices = df['Adj Close'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "3rLJvksindAX",
    "outputId": "c7864a99-bf99-4fd1-b972-9393195f78cb"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df.index, prices)\n",
    "ax.set(title=f\"{TICKER}'s Stock price\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi3POLQrnuYd"
   },
   "source": [
    "4. Define a function for transforming time series into a dataset for the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD9BNE3Go5J6"
   },
   "outputs": [],
   "source": [
    "def create_input_data(series, n_lags=1):\n",
    "  '''\n",
    "  Function for transforming time series into input acceptable by a multilayer perceptron.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  series : np.array\n",
    "  The time series to be transformed\n",
    "  n_lags : int\n",
    "  The number of lagged observations to consider as features\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  X : np.array\n",
    "  Array of features\n",
    "  y : np.array\n",
    "  Array of target\n",
    "  '''\n",
    "  X, y = [], []\n",
    "\n",
    "  for step in range(len(series) - n_lags):\n",
    "    end_step = step + n_lags\n",
    "    X.append(series[step:end_step])\n",
    "    y.append(series[end_step])\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pj6rBVvrZWN"
   },
   "source": [
    "5. Transform the considered time series into input for the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VHqZMm1rebm"
   },
   "outputs": [],
   "source": [
    "X, y = create_input_data(prices, N_LAGS)\n",
    "\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).float().unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eaAJd7srqp2"
   },
   "source": [
    "6. Create training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9adtdHurulH"
   },
   "outputs": [],
   "source": [
    "valid_ind = len(X) - VALID_SIZE\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "train_dataset = Subset(dataset, list(range(valid_ind)))\n",
    "valid_dataset = Subset(dataset, list(range(valid_ind, len(X))))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtwJWOm8sOHG"
   },
   "source": [
    "Inspect the observations from the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZ-ukInTsSbF",
    "outputId": "b7d2cd89-2468-4622-fd34-a64528b78597"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcjjhalzsXFx",
    "outputId": "462d634b-541e-4176-8aa5-c43c7dd9c235"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z-h6E8LscNt"
   },
   "source": [
    "Check the size of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1W3Fvpdsfax",
    "outputId": "2ceb07ea-b0d5-446a-e71a-e651b6bddf81"
   },
   "outputs": [],
   "source": [
    "print(f'Size of datasets - training: {len(train_loader.dataset)} | validation: {len(valid_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRnTCnvCssAk"
   },
   "source": [
    "7. Use naive forecast as a benchmark and evaluate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vagLv4LYsxPw",
    "outputId": "65f64ed4-5f66-4da8-92d2-05f087ce6a37"
   },
   "outputs": [],
   "source": [
    "naive_pred = prices[len(prices) - VALID_SIZE - 1:-1]\n",
    "y_valid = prices[len(prices) - VALID_SIZE:]\n",
    "\n",
    "naive_mse = mean_squared_error(y_valid, naive_pred)\n",
    "naive_rmse = np.sqrt(naive_mse)\n",
    "print(f\"Naive forecast - MSE: {naive_mse:.2f}, RMSE: {naive_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2_qvJzYtHcq"
   },
   "outputs": [],
   "source": [
    "# BONUS: Testing Linear Regression\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# X_train = X[:valid_ind, ]\n",
    "# y_train = y[:valid_ind]\n",
    "\n",
    "# X_valid = X[valid_ind:, ]\n",
    "# y_valid = y[valid_ind:]\n",
    "\n",
    "# lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = lin_reg.predict(X_valid)\n",
    "# lr_mse = mean_squared_error(y_valid, y_pred)\n",
    "# lr_rmse = np.sqrt(lr_mse)\n",
    "# print(f\"Linear Regression's forecast - MSE: {lr_mse:.2f}, RMSE: {lr_rmse:.2f}\")\n",
    "# print(f\"Linear Regression's coefficients: {lin_reg.coef_}\")\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.plot(y_valid, color='blue', label='Actual')\n",
    "# ax.plot(y_pred, color='red', label='Prediction')\n",
    "\n",
    "# ax.set(title=\"Linear Regression's Forecasts\", \n",
    "#        xlabel='Time', \n",
    "#        ylabel='Price ($)')\n",
    "# ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlBmbI7NuG1s"
   },
   "source": [
    "8. Define the network's architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwLhSnYduKPu"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size):\n",
    "    super(MLP, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_size, 8)\n",
    "    self.linear2 = nn.Linear(8, 4)\n",
    "    self.linear3 = nn.Linear(4, 1)\n",
    "    self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.linear2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.linear3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saj0xnNbuqUi"
   },
   "source": [
    "9. Instantiate the model, the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDhoUlhXuvOF"
   },
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = MLP(N_LAGS).to(device) \n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPDRfLKfu942",
    "outputId": "e98f9555-26c3-49cb-ada2-6aa4c6b1b657"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUofUVn-vA3u"
   },
   "source": [
    "10. Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wnnw8aiLyERM",
    "outputId": "28400669-f935-405e-cc82-bac0d9153602"
   },
   "outputs": [],
   "source": [
    "PRINT_EVERY = 50\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss_train = 0\n",
    "    running_loss_valid = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_hat = model(x_batch)\n",
    "        loss = loss_fn(y_batch, y_hat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_train += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for x_val, y_val in valid_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            y_hat = model(x_val)\n",
    "            loss = loss_fn(y_val, y_hat)\n",
    "            running_loss_valid += loss.item() * x_val.size(0)\n",
    "\n",
    "        epoch_loss_valid = running_loss_valid / len(valid_loader.dataset)\n",
    "\n",
    "        if epoch > 0 and epoch_loss_valid < min(valid_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), './mlp_checkpoint.pth')\n",
    "\n",
    "        valid_losses.append(epoch_loss_valid)\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"<{epoch}> - Train. loss: {epoch_loss_train:.2f} \\t Valid. loss: {epoch_loss_valid:.2f}\")\n",
    "\n",
    "print(f'Lowest loss recorded in epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print frequency for progress updates\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "train_losses, valid_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss_train = 0\n",
    "    running_loss_valid = 0\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Training phase\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to the appropriate device (CPU or GPU)\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = model(x_batch)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(y_batch, y_hat)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        running_loss_train += loss.item() * x_batch.size(0)\n",
    "\n",
    "    # Compute the average training loss for the epoch\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "\n",
    "    # Validation phase (no gradient computation)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for x_val, y_val in valid_loader:\n",
    "            # Move data to the appropriate device (CPU or GPU)\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = model(x_val)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(y_val, y_hat)\n",
    "\n",
    "            # Accumulate the validation loss\n",
    "            running_loss_valid += loss.item() * x_val.size(0)\n",
    "\n",
    "        # Compute the average validation loss for the epoch\n",
    "        epoch_loss_valid = running_loss_valid / len(valid_loader.dataset)\n",
    "\n",
    "        # Save the model if it has the lowest validation loss so far\n",
    "        if epoch > 0 and epoch_loss_valid < min(valid_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), './mlp_checkpoint.pth')\n",
    "\n",
    "        valid_losses.append(epoch_loss_valid)\n",
    "\n",
    "    # Print progress every PRINT_EVERY epochs\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"<{epoch}> - Train. loss: {epoch_loss_train:.2f} \\t Valid. loss: {epoch_loss_valid:.2f}\")\n",
    "\n",
    "# Print the epoch with the lowest validation loss\n",
    "print(f'Lowest loss recorded in epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vuXtIUK05mq"
   },
   "source": [
    "11. Plot the losses over epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "_Rv1XO0s085T",
    "outputId": "2bdd222d-e92c-403b-c164-ab6e391b69e5"
   },
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses)\n",
    "valid_losses = np.array(valid_losses)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_losses, color='blue', label='Training loss')\n",
    "ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "\n",
    "ax.set(title='Loss over epochs', \n",
    "       xlabel='Epoch', \n",
    "       ylabel='Loss')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAgDHdXV1c4_"
   },
   "source": [
    "12. Load the best model (with the lowest validation loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KZvGwMU1hxd",
    "outputId": "eb16282e-a2e2-4022-da95-2573841d02b0"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('mlp_checkpoint.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNYBdAEM1p28"
   },
   "source": [
    "13. Obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oRW_AaC1ssU"
   },
   "outputs": [],
   "source": [
    "y_pred, y_valid= [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    model.eval()\n",
    "    for x_val, y_val in valid_loader:\n",
    "        x_val = x_val.to(device)    \n",
    "        y_pred.append(model(x_val))\n",
    "        y_valid.append(y_val)\n",
    "\n",
    "y_pred = torch.cat(y_pred).numpy().flatten()\n",
    "y_valid = torch.cat(y_valid).numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0UGjSMK2Hsw"
   },
   "source": [
    "14. Evaluate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "id": "WVUrsxSQ2LGQ",
    "outputId": "6c486c4e-a9b9-48cb-f33d-a4a779a313df"
   },
   "outputs": [],
   "source": [
    "mlp_mse = mean_squared_error(y_valid, y_pred)\n",
    "mlp_rmse = np.sqrt(mlp_mse)\n",
    "print(f\"MLP's forecast - MSE: {mlp_mse:.2f}, RMSE: {mlp_rmse:.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_valid, color='blue', label= 'True')\n",
    "ax.plot(y_pred, color='red', label='Prediction')\n",
    "\n",
    "ax.set(title=\"Multilayer Perceptron's Forecasts\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uod__k4UJh4i"
   },
   "source": [
    "## Convolutional neural networks for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta86JxVfKUEF"
   },
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGXG258YKWiX",
    "outputId": "498660d9-14d9-4256-c2ad-f27669bbfd24"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, Subset\n",
    "from collections import OrderedDict\n",
    "from chapter_10_utils import create_input_data, custom_set_seed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(torch.__version__)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnlIpaESK6r7"
   },
   "source": [
    "2. Define the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izvPe91TK9Zy"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "TICKER = 'INTC'\n",
    "START_DATE = '2015-01-02'\n",
    "END_DATE = '2019-12-31'\n",
    "VALID_START = '2019-07-01'\n",
    "N_LAGS = 12\n",
    "\n",
    "# neural network \n",
    "BATCH_SIZE = 5\n",
    "N_EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsNlNV3hP2Fn"
   },
   "source": [
    "3. Download and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh9_cpwTP5dS"
   },
   "outputs": [],
   "source": [
    "df = yf.download(TICKER, \n",
    "                 start=START_DATE, \n",
    "                 end=END_DATE,\n",
    "                 progress=False)\n",
    "\n",
    "df = df.resample('W-MON').last()\n",
    "valid_size = df.loc[VALID_START:END_DATE].shape[0]\n",
    "prices = df['Adj Close'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "drAQZPGeQLBh",
    "outputId": "9eed08d9-f0b9-42cf-bb86-234827d8c176"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df.index, prices)\n",
    "ax.set(title=f\"{TICKER}'s Stock price\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLqy85UtQac0"
   },
   "source": [
    "4. Transform the time series into input for the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLw_BjKNQez0"
   },
   "outputs": [],
   "source": [
    "X, y = create_input_data(prices, N_LAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VozZC3LWQjlx"
   },
   "source": [
    "5. Obtain the naïve forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67CUWQs4Qmia",
    "outputId": "d9412d18-b61f-4792-a69f-7b90b0b34832"
   },
   "outputs": [],
   "source": [
    "naive_pred = prices[len(prices) - valid_size - 1:-1]\n",
    "y_valid = prices[len(prices) - valid_size:]\n",
    "\n",
    "naive_mse = mean_squared_error(y_valid, naive_pred)\n",
    "naive_rmse = np.sqrt(naive_mse)\n",
    "print(f\"Naive forecast - MSE: {naive_mse:.2f}, RMSE: {naive_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug2JQmDeQ9cG"
   },
   "source": [
    "6. Prepare the `DataLoader` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9Irsw9aRBBE"
   },
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "custom_set_seed(42)\n",
    "\n",
    "valid_ind = len(X) - valid_size\n",
    "\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).float().unsqueeze(dim=1)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "train_dataset = Subset(dataset, list(range(valid_ind)))\n",
    "valid_dataset = Subset(dataset, list(range(valid_ind, len(X))))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwQ5Y8zgRqRo"
   },
   "source": [
    "Check the size of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IcTSkSuRtMq",
    "outputId": "1789d836-a286-428a-c7a8-cf975f6210bf"
   },
   "outputs": [],
   "source": [
    "print(f'Size of datasets - training: {len(train_loader.dataset)} | validation: {len(valid_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDZCTMBER3y6"
   },
   "source": [
    "7. Define the CNN's architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4MUamBFR7Jo",
    "outputId": "094e6998-8980-42ad-c708-ca89dc61caea"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "  def forward(self, x):\n",
    "      return x.view(x.size()[0], -1)\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv_1', nn.Conv1d(1, 32, 3, padding=1)),\n",
    "    ('max_pool_1', nn.MaxPool1d(2)),\n",
    "    ('relu_1', nn.ReLU()),\n",
    "    ('flatten', Flatten()),\n",
    "    ('fc_1', nn.Linear(192, 50)),\n",
    "    ('relu_2', nn.ReLU()),\n",
    "    ('dropout_1', nn.Dropout(0.4)),\n",
    "    ('fc_2', nn.Linear(50, 1))\n",
    "]))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf28cKcPUDCd"
   },
   "source": [
    "8. Instantiate the model, the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqvBJbHsUIRJ"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALAr66nFUSKC"
   },
   "source": [
    "9. Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQNINcNxUUeN",
    "outputId": "7d626059-ba9d-4e32-9a50-a5486280ab81"
   },
   "outputs": [],
   "source": [
    "PRINT_EVERY = 50\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss_train = 0\n",
    "    running_loss_valid = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_batch = x_batch.to(device)\n",
    "        x_batch = x_batch.view(x_batch.shape[0], 1, N_LAGS)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_batch = y_batch.view(y_batch.shape[0], 1, 1)\n",
    "        y_hat = model(x_batch).view(y_batch.shape[0], 1, 1)\n",
    "        loss = torch.sqrt(loss_fn(y_batch, y_hat))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_train += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x_val, y_val in valid_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            x_val = x_val.view(x_val.shape[0], 1, N_LAGS)\n",
    "            y_val = y_val.to(device)\n",
    "            y_val = y_val.view(y_val.shape[0], 1, 1)\n",
    "            y_hat = model(x_val).view(y_val.shape[0], 1, 1)\n",
    "            loss = torch.sqrt(loss_fn(y_val, y_hat))\n",
    "            running_loss_valid += loss.item() * x_val.size(0)\n",
    "\n",
    "        epoch_loss_valid = running_loss_valid / len(valid_loader.dataset)\n",
    "\n",
    "        if epoch > 0 and epoch_loss_valid < min(valid_losses):\n",
    "           best_epoch = epoch\n",
    "           torch.save(model.state_dict(), './cnn_checkpoint.pth')\n",
    "\n",
    "        valid_losses.append(epoch_loss_valid)\n",
    "\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"<{epoch}> - Train. loss: {epoch_loss_train:.6f} \\t Valid. loss: {epoch_loss_valid:.6f}\")\n",
    "\n",
    "print(f'Lowest loss recorded in epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print frequency for progress updates\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss_train = 0\n",
    "    running_loss_valid = 0\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Training phase\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to the appropriate device (CPU or GPU)\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Reshape the input and output tensors\n",
    "        x_batch = x_batch.view(x_batch.shape[0], 1, N_LAGS)\n",
    "        y_batch = y_batch.view(y_batch.shape[0], 1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = model(x_batch).view(y_batch.shape[0], 1, 1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = torch.sqrt(loss_fn(y_batch, y_hat))\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        running_loss_train += loss.item() * x_batch.size(0)\n",
    "\n",
    "    # Compute the average training loss for the epoch\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "\n",
    "    # Validation phase (no gradient computation)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x_val, y_val in valid_loader:\n",
    "            # Move data to the appropriate device (CPU or GPU)\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            # Reshape the input and output tensors\n",
    "            x_val = x_val.view(x_val.shape[0], 1, N_LAGS)\n",
    "            y_val = y_val.view(y_val.shape[0], 1, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = model(x_val).view(y_val.shape[0], 1, 1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = torch.sqrt(loss_fn(y_val, y_hat))\n",
    "\n",
    "            # Accumulate the validation loss\n",
    "            running_loss_valid += loss.item() * x_val.size(0)\n",
    "\n",
    "        # Compute the average validation loss for the epoch\n",
    "        epoch_loss_valid = running_loss_valid / len(valid_loader.dataset)\n",
    "\n",
    "        # Save the model if it has the lowest validation loss so far\n",
    "        if epoch > 0 and epoch_loss_valid < min(valid_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), './cnn_checkpoint.pth')\n",
    "\n",
    "        valid_losses.append(epoch_loss_valid)\n",
    "\n",
    "    # Print progress every PRINT_EVERY epochs\n",
    "    if epoch % PRINT_EVERY == 0:\n",
    "        print(f\"<{epoch}> - Train. loss: {epoch_loss_train:.6f} \\t Valid. loss: {epoch_loss_valid:.6f}\")\n",
    "\n",
    "# Print the epoch with the lowest validation loss\n",
    "print(f'Lowest loss recorded in epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpQN_eJHYmOb"
   },
   "source": [
    "10. Plot the losses over epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "ccMkVRjfYphM",
    "outputId": "20bca332-873b-49b3-947e-a13d54c1e582"
   },
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses)\n",
    "valid_losses = np.array(valid_losses)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_losses, color='blue', label='Training loss')\n",
    "ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "\n",
    "ax.set(title=\"Loss over epochs\", \n",
    "       xlabel='Epoch', \n",
    "       ylabel='Loss')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyRTlWUJZIC8"
   },
   "source": [
    "11. Load the best model (with the lowest validation loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOor3Pd4ZNN8",
    "outputId": "143c33ff-b9c4-43c7-86a3-e0407d8c5816"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('cnn_checkpoint.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxNostS2ZWwi"
   },
   "source": [
    "12. Obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjYfIYPiZZo3"
   },
   "outputs": [],
   "source": [
    "y_pred, y_valid = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for x_val, y_val in valid_loader:\n",
    "        x_val = x_val.to(device)\n",
    "        x_val = x_val.view(x_val.shape[0], 1, N_LAGS)\n",
    "        y_pred.append(model(x_val))\n",
    "        y_valid.append(y_val)\n",
    "\n",
    "y_pred = torch.cat(y_pred).numpy().flatten()\n",
    "y_valid = torch.cat(y_valid).numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNY3TRElaKrd"
   },
   "source": [
    "13. Evaluate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "id": "jJT-W6XfaNnm",
    "outputId": "1e5ed949-9c8a-4561-df8d-eaddd8f2b85c"
   },
   "outputs": [],
   "source": [
    "cnn_mse = mean_squared_error(y_valid, y_pred)\n",
    "cnn_rmse = np.sqrt(cnn_mse)\n",
    "print(f\"CNN's forecast - MSE: {cnn_mse:.2f}, RMSE: {cnn_rmse:.2f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_valid, color='blue', label='Actual')\n",
    "ax.plot(y_pred, color='red', label='Prediction')\n",
    "#ax.plot(naive_pred, color='green', label='Naïve')\n",
    "\n",
    "ax.set(title=\"CNN's Forecasts\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rML7N6A5e8Pa"
   },
   "source": [
    "## Recurrent neural networks for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqSuiDENo8a0"
   },
   "source": [
    "1. Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_glzpxUo_ZE",
    "outputId": "94af6637-7281-4602-d778-24d619d74c44"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, Subset\n",
    "from collections import OrderedDict\n",
    "from chapter_10_utils import create_input_data, custom_set_seed\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE8k_Jvlp-9E"
   },
   "source": [
    "2. Define the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykIPxRr2qBhL"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "TICKER = 'INTC'\n",
    "START_DATE = '2010-01-02'\n",
    "END_DATE = '2019-12-31'\n",
    "VALID_START = '2019-07-01'\n",
    "N_LAGS = 12\n",
    "\n",
    "# neural network \n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y51rh19KqQ6D"
   },
   "source": [
    "3. Download and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QA_aQ4SqT16"
   },
   "outputs": [],
   "source": [
    "df = yf.download(TICKER, \n",
    "                 start=START_DATE, \n",
    "                 end=END_DATE,\n",
    "                 progress=False)\n",
    "\n",
    "df = df.resample('W-MON').last()\n",
    "valid_size = df.loc[VALID_START:END_DATE].shape[0]\n",
    "prices = df['Adj Close'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "caUtSDqLqm_1",
    "outputId": "f5b0f1f9-ad85-45bb-da01-368fa6664a71"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df.index, prices)\n",
    "ax.set(title=f\"{TICKER}'s Stock price\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg6FJ0t6q5sY"
   },
   "source": [
    "4. Scale the time series of prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WREdeyH1q9B2"
   },
   "outputs": [],
   "source": [
    "valid_ind = len(prices) - valid_size\n",
    "minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "prices_train = prices[:valid_ind]\n",
    "prices_valid = prices[valid_ind:]\n",
    "\n",
    "minmax.fit(prices_train)\n",
    "\n",
    "prices_train = minmax.transform(prices_train)\n",
    "prices_valid = minmax.transform(prices_valid)\n",
    "\n",
    "prices_scaled = np.concatenate((prices_train, \n",
    "                                prices_valid)).flatten()\n",
    "#plt.plot(prices_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2gtRA3vrnzq"
   },
   "source": [
    "5. Transform the time series into input for the RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6uOlQB8rsda"
   },
   "outputs": [],
   "source": [
    "X, y = create_input_data(prices_scaled, N_LAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Icesn_Zr1JR"
   },
   "source": [
    "6. Obtain the naïve forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Adq3fl9Nr4qy",
    "outputId": "88aefc7c-cc0c-4760-e118-acb743956a3d"
   },
   "outputs": [],
   "source": [
    "naive_pred = prices[len(prices)-valid_size-1:-1]\n",
    "y_valid = prices[len(prices)-valid_size:]\n",
    "\n",
    "naive_mse = mean_squared_error(y_valid, naive_pred)\n",
    "naive_rmse = np.sqrt(naive_mse)\n",
    "print(f\"Naive forecast - MSE: {naive_mse:.4f}, RMSE: {naive_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4SrNzapsPVm"
   },
   "source": [
    "7. Prepare the `DataLoader` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_CTpdp7sS2q"
   },
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "custom_set_seed(42)\n",
    "\n",
    "valid_ind = len(X) - valid_size\n",
    "\n",
    "X_tensor = torch.from_numpy(X).float().reshape(X.shape[0], \n",
    "                                               X.shape[1], \n",
    "                                               1)\n",
    "y_tensor = torch.from_numpy(y).float().reshape(X.shape[0], 1)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "train_dataset = Subset(dataset, list(range(valid_ind)))\n",
    "valid_dataset = Subset(dataset, list(range(valid_ind, len(X))))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcC9W7QatAqP"
   },
   "source": [
    "Check the size of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkH6M6GJtD-M",
    "outputId": "17b31851-8936-4f62-c3ad-a39f491941bd"
   },
   "outputs": [],
   "source": [
    "print(f'Size of datasets - training: {len(train_loader.dataset)} | validation: {len(valid_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViGKeTUQtOYv"
   },
   "source": [
    "8. Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk5T0SdAtRYv"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, n_layers, output_size):\n",
    "      super(RNN, self).__init__()\n",
    "      self.rnn = nn.RNN(input_size, hidden_size, \n",
    "                        n_layers, batch_first=True,\n",
    "                        nonlinearity='relu')\n",
    "      self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "  def forward(self, x):\n",
    "      output, _ = self.rnn(x)\n",
    "      output = self.fc(output[:,-1,:]) \n",
    "      return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vksH5tuYt3oe"
   },
   "source": [
    "9. Instantiate the model, the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wlg4Hh1Dt8vx"
   },
   "outputs": [],
   "source": [
    "model = RNN(input_size=1, hidden_size=6, \n",
    "            n_layers=1, output_size=1).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzxJlvLouOOK"
   },
   "source": [
    "10. Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IK4GMrNyuRJc",
    "outputId": "d9ef440d-3e9b-4fc7-8484-6e3676d36db1"
   },
   "outputs": [],
   "source": [
    "PRINT_EVERY = 10\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss_train = 0\n",
    "    running_loss_valid = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_hat = model(x_batch)\n",
    "        loss = torch.sqrt(loss_fn(y_batch, y_hat))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_train += loss.item() * x_batch.size(0)\n",
    "\n",
    "    epoch_loss_train = running_loss_train / len(train_loader)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x_val, y_val in valid_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            y_hat = model(x_val)\n",
    "            loss = torch.sqrt(loss_fn(y_val, y_hat))\n",
    "            running_loss_valid += loss.item() * x_val.size(0)\n",
    "\n",
    "        epoch_loss_valid = running_loss_valid / len(valid_loader.dataset)\n",
    "\n",
    "        if epoch > 0 and epoch_loss_valid < min(valid_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), './rnn_checkpoint.pth')\n",
    "\n",
    "        valid_losses.append(epoch_loss_valid)\n",
    "\n",
    "        if epoch % PRINT_EVERY == 0:\n",
    "            print(f\"<{epoch}> - Train. loss: {epoch_loss_train:.4f} \\t Valid. loss: {epoch_loss_valid:.4f}\")\n",
    "\n",
    "print(f'Lowest loss recorded in epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print frequency for progress updates\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss_train = 0\n",
    "    running_loss_valid = 0\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Training phase\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to the appropriate device (CPU or GPU)\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat = model(x_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = torch.sqrt(loss_fn(y_batch, y_hat))\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss\n",
    "        running_loss_train += loss.item() * x_batch.size(0)\n",
    "\n",
    "    # Compute the average training loss for the epoch\n",
    "    epoch_loss_train = running_loss_train / len(train_loader)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "\n",
    "    # Validation phase (no gradient computation)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x_val, y_val in valid_loader:\n",
    "            # Move data to the appropriate device (CPU or GPU)\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = model(x_val)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = torch.sqrt(loss_fn(y_val, y_hat))\n",
    "\n",
    "            # Accumulate the validation loss\n",
    "            running_loss_valid += loss.item() * x_val.size(0)\n",
    "\n",
    "        # Compute the average validation loss for the epoch\n",
    "        epoch_loss_valid = running_loss_valid / len(valid_loader.dataset)\n",
    "\n",
    "        # Save the model if it has the lowest validation loss so far\n",
    "        if epoch > 0 and epoch_loss_valid < min(valid_losses):\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), './rnn_checkpoint.pth')\n",
    "\n",
    "        valid_losses.append(epoch_loss_valid)\n",
    "\n",
    "        # Print progress every PRINT_EVERY epochs\n",
    "        if epoch % PRINT_EVERY == 0:\n",
    "            print(f\"<{epoch}> - Train. loss: {epoch_loss_train:.4f} \\t Valid. loss: {epoch_loss_valid:.4f}\")\n",
    "\n",
    "# Print the epoch with the lowest validation loss\n",
    "print(f'Lowest loss recorded in epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cfDk7CUyI8z"
   },
   "source": [
    "11. Plot the losses over epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "IOKpSuJFyMYu",
    "outputId": "4f4d0a91-6f33-4c01-eacb-091f5b8b29a2"
   },
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses)\n",
    "valid_losses = np.array(valid_losses)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_losses, color='blue', label='Training loss')\n",
    "ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "\n",
    "ax.set(title=\"Loss over epochs\", \n",
    "       xlabel='Epoch', \n",
    "       ylabel='Loss')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF5DrQ59yqZk"
   },
   "source": [
    "12. Load the best model (with the lowest validation loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fw2yReDByvWS",
    "outputId": "1e402cc5-15b2-40d4-bcb1-04316e56a155"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('rnn_checkpoint.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTX26D4Iy44_"
   },
   "source": [
    "13. Obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRqU94wny73m"
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for x_val, y_val in valid_loader:\n",
    "        x_val = x_val.to(device)\n",
    "        y_hat = model(x_val)\n",
    "        y_pred.append(y_hat)\n",
    "\n",
    "y_pred = torch.cat(y_pred).numpy()\n",
    "y_pred = minmax.inverse_transform(y_pred).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaTjCKk3zTse"
   },
   "source": [
    "14. Evaluate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "id": "c99w0d-XzWeb",
    "outputId": "89275e7f-6fd4-4c49-fe2a-ee99fe7f31cb"
   },
   "outputs": [],
   "source": [
    "rnn_mse = mean_squared_error(y_valid, y_pred)\n",
    "rnn_rmse = np.sqrt(rnn_mse)\n",
    "print(f\"RNN's forecast - MSE: {rnn_mse:.4f}, RMSE: {rnn_rmse:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_valid, color='blue', label='Actual')\n",
    "ax.plot(y_pred, color='red', label='RNN')\n",
    "ax.plot(naive_pred, color='green', label='Naïve')\n",
    "\n",
    "ax.set(title=\"RNN's Forecasts\", \n",
    "       xlabel='Time', \n",
    "       ylabel='Price ($)')\n",
    "ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter 10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
